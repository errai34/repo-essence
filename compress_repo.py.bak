#!/usr/bin/env python3
"""
Produce a single compressed XML for tutorials/demos/examples and their dependencies,
preserving only essential code lines and summarizing the rest with SentenceTransformer.
"""

import re
import xml.etree.ElementTree as ET
from pathlib import Path
from xml.etree.ElementTree import Element, SubElement, tostring

import tiktoken
from sentence_transformers import SentenceTransformer

# -------------------------------------------------------------------
# CONFIG
# -------------------------------------------------------------------
MAX_TOKEN_LIMIT = 100_000  # ~100k tokens
TUTORIAL_DEMO_PATH_PATTERN = re.compile(
    r".*/(tutorials?|examples?|demos?)/.*", re.IGNORECASE
)
ALLOW_PARTIAL_FOR_FORCED = True

# Regexes for lines that we want to preserve explicitly
PRESERVE_REGEXES = [
    re.compile(r"^\s*def\s+\w+"),  # def function()
    re.compile(r"^\s*class\s+\w+"),  # class MyClass
    re.compile(r"^\s*from\s+\S+\s+import"),  # from X import Y
    re.compile(r"^\s*import\s+\S+"),  # import X
    re.compile(r"^\s*@\w+(\(.*?\))?"),  # @decorator(...)
]

# For summarization of non-essential lines
SUMMARY_CHUNK_SIZE = 15  # how many lines to batch before summarizing
SUMMARY_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"


# -------------------------------------------------------------------
# DEPENDENCY AND CODE UTILS
# -------------------------------------------------------------------
def find_local_imports(code_text: str):
    """
    Naive approach: returns set of local modules that might be imported
    from lines like:
      import something
      from something import ...
    """
    local_mods = set()
    import_pat = re.compile(r"^(?:from|import)\s+(\S+)", re.M)
    for m in import_pat.findall(code_text):
        modroot = m.split(".")[0]
        local_mods.add(modroot)
    return local_mods


def summarize_lines(lines, model):
    """
    Summarize a chunk of lines using a SentenceTransformer.
    For demonstration, we'll produce a single 'summary line' for the entire chunk.

    In practice, you could do more advanced chunk-based summarization:
    e.g. embed lines, cluster them, pick key sentences, etc.
    """
    text_block = "\n".join(lines)
    # For a short demonstration, let's do a naive "embedding & top sentence" approach.
    # Real usage might use a separate summarization method or LLM approach.
    if not lines:
        return "# (no lines to summarize)"

    # We'll just embed the entire block and pretend we do something more sophisticated.
    _ = model.encode([text_block])  # embedding (not really used in this example)
    summary = lines[0].strip()
    if len(summary) > 60:
        summary = summary[:60] + "..."
    return f"# Summary of block: {summary}"


def preserve_or_summarize(code_text: str, model) -> str:
    """
    1) Keep lines that match PRESERVE_REGEXES.
    2) For lines that do not match, group them into chunks and produce a single summary line each chunk.
    """
    out = []
    chunk_buffer = []

    def flush_chunk():
        """Summarize the chunk_buffer, append one summary line."""
        if chunk_buffer:
            out.append(summarize_lines(chunk_buffer, model))
            chunk_buffer.clear()

    for line in code_text.split("\n"):
        stripped = line.strip()
        if any(rx.match(stripped) for rx in PRESERVE_REGEXES):
            # If there's something in chunk_buffer, flush it first
            if chunk_buffer:
                flush_chunk()
            out.append(line)
        else:
            chunk_buffer.append(line)
            # if chunk is big enough, summarize
            if len(chunk_buffer) >= SUMMARY_CHUNK_SIZE:
                flush_chunk()

    # Summarize any leftover
    flush_chunk()

    return "\n".join(out)


# -------------------------------------------------------------------
# TOKEN HELPERS
# -------------------------------------------------------------------
def count_tokens(text: str, encoder) -> int:
    """Approximate token usage with a tiktoken encoder."""
    return len(encoder.encode(text))


def partial_fit_code(
    code_text: str,
    current_tokens: int,
    closing_tokens: int,
    token_limit: int,
    encoder,
    prefix_xml: str,
    suffix_xml: str,
) -> str:
    """
    Binary search for maximum lines we can include from code_text
    without exceeding token_limit, given prefix_xml + suffix_xml overhead.
    """
    lines = code_text.split("\n")
    low, high = 0, len(lines)
    best_fit = ""

    while low <= high:
        mid = (low + high) // 2
        snippet = "\n".join(lines[:mid])
        test_xml = prefix_xml + snippet + suffix_xml
        needed = current_tokens + count_tokens(test_xml, encoder) + closing_tokens
        if needed <= token_limit:
            best_fit = snippet
            low = mid + 1
        else:
            high = mid - 1

    return best_fit


# -------------------------------------------------------------------
# DATA STRUCTURES FOR REPO
# -------------------------------------------------------------------
class FileNode:
    def __init__(self, path: str, content: str):
        self.path = path
        self.content = content
        self.is_forced_relevant = False
        self.local_imports = set()
        self.already_included = False


class DirectoryNode:
    def __init__(self, name: str):
        self.name = name
        self.subdirs = {}
        self.files = []


def insert_file_into_dir(root_dir: DirectoryNode, path_str: str, content: str):
    parts = Path(path_str).parts
    curr = root_dir
    for p in parts[:-1]:
        if p not in curr.subdirs:
            curr.subdirs[p] = DirectoryNode(p)
        curr = curr.subdirs[p]
    fn = FileNode(path_str, content)
    fn.local_imports = find_local_imports(content)
    curr.files.append(fn)


# -------------------------------------------------------------------
# XML PARSING (with fallback repair)
# -------------------------------------------------------------------
def safely_parse_xml(xml_path: str):
    """
    Try to parse XML normally. If it fails, attempt minimal "repair":
    e.g. find <root>...</root> with a regex and parse that substring.
    """
    raw = Path(xml_path).read_text(encoding="utf-8", errors="replace")

    # Attempt normal parse
    try:
        tree = ET.ElementTree(ET.fromstring(raw))
        return tree
    except ET.ParseError:
        pass

    # Fallback: find <root>...</root> or <repository_files>...</repository_files> etc.
    # We can do something naive here:
    m = re.search(r"(<root\b.*</root>)", raw, flags=re.DOTALL)
    if not m:
        # Try <repository_files> or something else
        m = re.search(
            r"(<repository_files\b.*</repository_files>)", raw, flags=re.DOTALL
        )
    if not m:
        raise ValueError(
            "Could not find any recognizable <root>...</root> in the file."
        )

    snippet = m.group(1)
    try:
        tree = ET.ElementTree(ET.fromstring(snippet))
        return tree
    except ET.ParseError as e:
        raise ValueError(f"Could not parse even the fallback snippet: {e}")


def parse_merged_xml(merged_xml_path: str) -> DirectoryNode:
    tree = safely_parse_xml(merged_xml_path)
    root_elem = tree.getroot()

    topdir = DirectoryNode("_REPO_ROOT_")

    # We expect <file path="..."> elements, possibly nested
    # For simplicity, we'll do a deep search
    for fnode in root_elem.findall(".//file"):
        path = fnode.get("path", "")
        text_content = fnode.text or ""
        insert_file_into_dir(topdir, path, text_content)

    return topdir


# -------------------------------------------------------------------
# MARK FORCED RELEVANT
# -------------------------------------------------------------------
def mark_forced_relevant(dir_node: DirectoryNode):
    """
    Mark files if their path matches TUTORIAL_DEMO_PATH_PATTERN (tutorial/demo/example).
    """
    for f in dir_node.files:
        if TUTORIAL_DEMO_PATH_PATTERN.search(f.path):
            f.is_forced_relevant = True
    for sd in dir_node.subdirs.values():
        mark_forced_relevant(sd)


def build_local_mod_index(dir_node: DirectoryNode):
    """
    Return a dict {modname -> list[FileNode]} if there's a file "modname.py"
    or folder "modname/__init__.py".
    """
    from collections import defaultdict

    mod_index = defaultdict(list)

    def walk(dn):
        for f in dn.files:
            p = Path(f.path)
            if p.stem == "__init__":
                # mod name is parent folder
                mod_name = p.parent.name
                mod_index[mod_name].append(f)
            else:
                # mod name is the file stem
                mod_name = p.stem
                mod_index[mod_name].append(f)
        for sdn in dn.subdirs.values():
            walk(sdn)

    walk(dir_node)
    return dict(mod_index)


def resolve_forced_dependencies(dir_node: DirectoryNode):
    """
    For any forced-relevant file, BFS on its local imports. Mark dependencies as forced too.
    """
    mod_index = build_local_mod_index(dir_node)

    # Gather all files in a list
    all_files = []

    def gather_files(dnode):
        for f in dnode.files:
            all_files.append(f)
        for sd in dnode.subdirs.values():
            gather_files(sd)

    gather_files(dir_node)

    from collections import deque

    queue = deque([f for f in all_files if f.is_forced_relevant])
    visited = set(f.path for f in queue)

    while queue:
        curf = queue.popleft()
        for modnm in curf.local_imports:
            if modnm in mod_index:
                for dep_fnode in mod_index[modnm]:
                    if dep_fnode.path not in visited:
                        dep_fnode.is_forced_relevant = True
                        visited.add(dep_fnode.path)
                        queue.append(dep_fnode)


# -------------------------------------------------------------------
# FINAL XML BUILDER
# -------------------------------------------------------------------
class RepoToXMLBuilder:
    def __init__(self, root_dir: DirectoryNode, token_limit: int = MAX_TOKEN_LIMIT):
        self.root_dir = root_dir
        self.token_limit = token_limit
        self.encoder = tiktoken.get_encoding("cl100k_base")  # choose your encoder
        self.root_elem = Element("root")
        self.current_tokens = count_tokens("<root>", self.encoder)
        self.closing_tokens = count_tokens("</root>", self.encoder)
        # Load summarizer
        self.summarizer_model = SentenceTransformer(SUMMARY_MODEL_NAME)

    def build_xml(self) -> str:
        """
        Build the compressed, single XML with forced relevant files (tutorials, etc.)
        plus forced dependencies. If any forced file is too large, partially compress it.
        """
        self._recurse_forced(self.root_dir, self.root_elem, is_top=True)
        final_str = tostring(self.root_elem, encoding="unicode")
        return final_str

    def _recurse_forced(self, dir_node: DirectoryNode, parent_elem, is_top=False):
        if not is_top:
            d_elem = SubElement(parent_elem, "directory", {"name": dir_node.name})
            my_parent = d_elem
        else:
            my_parent = parent_elem

        # Add forced files
        for f in dir_node.files:
            if f.is_forced_relevant:
                self._try_add_forced_file(my_parent, f)

        # Recurse subdirs
        for sd in dir_node.subdirs.values():
            self._recurse_forced(sd, my_parent, is_top=False)

    def _try_add_forced_file(self, parent_elem, filenode: FileNode):
        # 1) Pre-filter: preserve lines or summarize with the SentenceTransformer
        filtered_code = preserve_or_summarize(filenode.content, self.summarizer_model)

        # 2) Attempt to add uncompressed (i.e., fully filtered_code)
        file_elem = Element("file", {"path": filenode.path, "forced": "true"})
        code_elem = SubElement(file_elem, "code")
        code_elem.text = filtered_code

        test_xml = tostring(file_elem, encoding="unicode")
        needed = (
            self.current_tokens
            + count_tokens(test_xml, self.encoder)
            + self.closing_tokens
        )
        if needed <= self.token_limit:
            # fits
            parent_elem.append(file_elem)
            self.current_tokens += count_tokens(test_xml, self.encoder)
        else:
            # partial fallback
            if ALLOW_PARTIAL_FOR_FORCED:
                prefix_xml = f'<file path="{filenode.path}" forced="partial"><code>'
                suffix_xml = "</code></file>"
                partial_text = partial_fit_code(
                    filtered_code,
                    self.current_tokens,
                    self.closing_tokens,
                    self.token_limit,
                    self.encoder,
                    prefix_xml,
                    suffix_xml,
                )
                if partial_text:
                    file_elem2 = Element(
                        "file", {"path": filenode.path, "forced": "partial"}
                    )
                    code2 = SubElement(file_elem2, "code")
                    code2.text = partial_text
                    final_str = tostring(file_elem2, encoding="unicode")
                    final_toks = count_tokens(final_str, self.encoder)
                    if (
                        self.current_tokens + final_toks + self.closing_tokens
                        <= self.token_limit
                    ):
                        parent_elem.append(file_elem2)
                        self.current_tokens += final_toks
            # else skip entirely


# -------------------------------------------------------------------
# MAIN CLI
# -------------------------------------------------------------------
def main():
    import argparse

    ap = argparse.ArgumentParser(
        description="Produce single compressed XML with tutorials/demos/examples from a merged repo XML."
    )
    ap.add_argument(
        "merged_xml", help="Path to merged representation file (the big .xml)."
    )
    ap.add_argument(
        "--token-limit",
        type=int,
        default=MAX_TOKEN_LIMIT,
        help="Max tokens for final output (default=100k).",
    )
    args = ap.parse_args()

    # 1) Build in-memory directory structure
    dir_tree = parse_merged_xml(args.merged_xml)

    # 2) Mark forced relevant by path
    mark_forced_relevant(dir_tree)

    # 3) Include forced dependencies
    resolve_forced_dependencies(dir_tree)

    # 4) Build final compressed XML
    builder = RepoToXMLBuilder(dir_tree, token_limit=args.token_limit)
    final_xml = builder.build_xml()

    print(final_xml)


if __name__ == "__main__":
    main()
